{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Attention Network (GAT) Layer:\n",
        "\n",
        "A GAT layer uses **self-attention on graphs** to weigh the importance of neighboring nodes. Here is the full pipeline:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Linear Transformation**\n",
        "\n",
        "Apply a learnable linear transformation to each input node feature:\n",
        "\n",
        "$\n",
        "h_i' = \\mathbf{W} h_i\n",
        "$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $\\mathbf{W} \\in \\mathbb{R}^{F' \\times F} $ is a learnable weight matrix.\n",
        "- $ h_i \\in \\mathbb{R}^{F} $: original feature vector of node \\( i \\).\n",
        "- $ h_i' \\in \\mathbb{R}^{F'} $: transformed feature vector.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Attention Coefficients**\n",
        "\n",
        "Compute the unnormalized attention score between node \\( i \\) and its neighbor \\( j \\):\n",
        "\n",
        "$\n",
        "e_{ij} = \\text{LeakyReLU}\\left( \\vec{a}^{\\top} \\left[ \\mathbf{W} h_i \\, \\| \\, \\mathbf{W} h_j \\right] \\right)\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ \\vec{a} \\in \\mathbb{R}^{2F'} $ is a learnable attention vector.\n",
        "- $ \\| $ denotes concatenation.\n",
        "- $ e_{ij} $ is the raw (unnormalized) attention coefficient.\n",
        "\n",
        "$$\n",
        "\\text{LeakyReLU}(x) =\n",
        "\\begin{cases}\n",
        "x & \\text{if } x > 0 \\\\\n",
        "0.01 x & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "LeakyReLu will adds non-linearity → allows the model to learn complex patterns and Prevents dead neurons (unlike regular ReLU)\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Softmax Normalization**\n",
        "\n",
        "Normalize the attention scores across neighbors of node \\( i \\):\n",
        "\n",
        "$\n",
        "\\alpha_{ij} = \\frac{ \\exp(e_{ij}) }{ \\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik}) }\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $\\mathcal{N}(i) $ denotes the set of neighbors of node \\( i \\).\n",
        "- $ \\alpha_{ij} \\in [0,1] $ represents the normalized attention coefficient from node \\( i \\) to node \\( j \\).\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Weighted Sum of Neighbor Features**\n",
        "\n",
        "Aggregate the transformed neighbor features weighted by attention:\n",
        "\n",
        "$\n",
        "h_i^{\\text{out}} = \\sigma\\left( \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} \\cdot \\mathbf{W} h_j \\right)\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ \\sigma $ is a non-linear activation function (e.g., ELU or LeakyReLU).\n",
        "- $ h_i^{\\text{out}} $ is the output embedding for node \\( i \\) after one GAT layer.\n",
        "\n",
        "---\n",
        "\n",
        "*This process allows each node to attend differently to each of its neighbors, enabling adaptive feature aggregation.*\n"
      ],
      "metadata": {
        "id": "bSZvCNBddcXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch_geometric"
      ],
      "metadata": {
        "id": "mJOY5QhNznob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1oarJ2wzVMt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GATLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GATLayer, self).__init__()\n",
        "\n",
        "  def forward(self, input, adj):\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "aRT82ejJzlRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"We will take input of size 3×5 which is multimplied by W (learned parameter) of size 5×2.\n",
        "it transorform each node's 5 feature into 2. Thus we get matrix of size 3×2\"\"\"\n",
        "\n",
        "in_features = 5\n",
        "out_features = 2\n",
        "nb_nodes = 3\n",
        "\n",
        "#xavier parameter initialization\n",
        "\"\"\"If the W is too large or small there is a possiblity of exploding/vanishing after applying the activation function,\n",
        "inorder to avoid this we use xavier uniform which maintaince a stable varience of activations through out the neural network layer.\"\"\"\n",
        "W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
        "input = torch.randn(nb_nodes, in_features)\n",
        "\n",
        "#Linear transformation\n",
        "h = torch.mm(input, W)\n",
        "N= h.size()[0]\n",
        "print(h.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQq_6UES0ZLV",
        "outputId": "fe637875-2fc1-417b-a162-3454f5f705c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
        "nn.init.xavier_uniform_(a.data, gain=1.414)\n",
        "print(a.shape)\n",
        "\n",
        "leakyrelu = nn.LeakyReLU(0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzJp_ypC3e5Z",
        "outputId": "7bfbc3ce-aca6-410f-9841-64066260e67f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * out_features)\n",
        "a_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti6Zkszr8dcF",
        "outputId": "abc798ff-522a-4213-98f9-71c3694e2ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0605, -0.3668,  0.0605, -0.3668],\n",
              "         [ 0.0605, -0.3668,  1.3190, -0.5537],\n",
              "         [ 0.0605, -0.3668, -1.0470, -0.0665]],\n",
              "\n",
              "        [[ 1.3190, -0.5537,  0.0605, -0.3668],\n",
              "         [ 1.3190, -0.5537,  1.3190, -0.5537],\n",
              "         [ 1.3190, -0.5537, -1.0470, -0.0665]],\n",
              "\n",
              "        [[-1.0470, -0.0665,  0.0605, -0.3668],\n",
              "         [-1.0470, -0.0665,  1.3190, -0.5537],\n",
              "         [-1.0470, -0.0665, -1.0470, -0.0665]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = leakyrelu(torch.matmul(a_input, a).squeeze(2))\n",
        "e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3UJXdK39H_U",
        "outputId": "08ec12ea-c643-4bd2-e953-9584b9114367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0372, -0.1255,  0.0574],\n",
              "        [-0.2840, -0.3723, -0.2353],\n",
              "        [ 1.0844,  0.6429,  1.3281]], grad_fn=<LeakyReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(a_input, a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY07tkiG9UB_",
        "outputId": "d4d1f630-c09a-4abf-fbd4-0176bebb0426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1862],\n",
              "         [-0.6277],\n",
              "         [ 0.0574]],\n",
              "\n",
              "        [[-1.4199],\n",
              "         [-1.8614],\n",
              "         [-1.1763]],\n",
              "\n",
              "        [[ 1.0844],\n",
              "         [ 0.6429],\n",
              "         [ 1.3281]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(a_input, a).squeeze(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgGSWzOm9iMi",
        "outputId": "679e266b-7425-4894-c213-1ef63b0ef4bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1862, -0.6277,  0.0574],\n",
              "        [-1.4199, -1.8614, -1.1763],\n",
              "        [ 1.0844,  0.6429,  1.3281]], grad_fn=<SqueezeBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Masked Attention"
      ],
      "metadata": {
        "id": "Nywvafpm-PIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adj = torch.randint(2, (3,3))\n",
        "zero_vec  = -9e15*torch.ones_like(e)\n",
        "zero_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZmxPS84-DTu",
        "outputId": "c7796d2e-4033-43d0-b2a3-b94874919618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
              "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
              "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention = torch.where(adj>0, e, zero_vec)\n",
        "print(adj,\"\\n\")\n",
        "print(e,\"\\n\")\n",
        "print(zero_vec)\n",
        "\n",
        "print(\"\\n\",attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYy5J_yF-W2I",
        "outputId": "ef601db2-d138-4bd4-f938-92d5ad3d8af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 1, 1],\n",
            "        [1, 0, 1],\n",
            "        [1, 0, 1]]) \n",
            "\n",
            "tensor([[-0.0372, -0.1255,  0.0574],\n",
            "        [-0.2840, -0.3723, -0.2353],\n",
            "        [ 1.0844,  0.6429,  1.3281]], grad_fn=<LeakyReluBackward0>) \n",
            "\n",
            "tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
            "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
            "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])\n",
            "\n",
            " tensor([[-3.7241e-02, -1.2554e-01,  5.7426e-02],\n",
            "        [-2.8398e-01, -9.0000e+15, -2.3526e-01],\n",
            "        [ 1.0844e+00, -9.0000e+15,  1.3281e+00]], grad_fn=<WhereBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention = F.softmax(attention, dim=1)\n",
        "h_prime = torch.matmul(attention, h)"
      ],
      "metadata": {
        "id": "DyhjzPi8BQA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eogr9b-BtOy",
        "outputId": "6d0b733f-6877-4ba2-fe10-8bd2af5914fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3317, 0.3037, 0.3646],\n",
              "        [0.4878, 0.0000, 0.5122],\n",
              "        [0.4394, 0.0000, 0.5606]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h_prime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU35DHzZBzx6",
        "outputId": "10b3e2dd-ee08-4e11-91bc-a0999d5566d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0388, -0.3141],\n",
              "        [-0.5067, -0.2130],\n",
              "        [-0.5604, -0.1984]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il0IR0baB1SR",
        "outputId": "6fe448dc-205d-44ce-b227-4bae766aa102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0605, -0.3668],\n",
              "        [ 1.3190, -0.5537],\n",
              "        [-1.0470, -0.0665]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GATLayer(nn.Module):\n",
        "  def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "    super(GATLayer, self).__init__()\n",
        "    self.dropout = dropout        # drop prob = 0.6\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.alpha = alpha          # LeakyReLU with negative input slope, alpha = 0.2\n",
        "    self.concat = concat         # conacat = True for all layers except the output layer.\n",
        "\n",
        "    # Xavier Initialization of Weights\n",
        "    # Alternatively use weights_init to apply weights of choice\n",
        "    self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "    nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "\n",
        "    self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
        "    nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "    # LeakyReLU\n",
        "    self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "  def forward(self, input, adj):\n",
        "    # Linear Transformation\n",
        "    h = torch.mm(input, self.W) # matrix multiplication\n",
        "    N = h.size()[0]\n",
        "    print(N)\n",
        "\n",
        "    # Attention Mechanism\n",
        "    a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "    e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "\n",
        "    # Masked Attention\n",
        "    zero_vec  = -9e15*torch.ones_like(e)\n",
        "    attention = torch.where(adj > 0, e, zero_vec)\n",
        "\n",
        "    attention = F.softmax(attention, dim=1)\n",
        "    attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "    h_prime   = torch.matmul(attention, h)\n",
        "\n",
        "    if self.concat:\n",
        "      return F.elu(h_prime)\n",
        "    else:\n",
        "      return h_prime"
      ],
      "metadata": {
        "id": "BmpEVqzwTAow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using Cora data set"
      ],
      "metadata": {
        "id": "sGZ-cHxxT0a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "\"\"\"When you pass the root='/tmp/Cora' and name='Cora', it looks inside '/tmp/Cora' for the dataset files.\n",
        "If the Cora dataset isn’t already downloaded, it automatically downloads it from the PyTorch Geometric repo or related source.\"\"\"\n",
        "name_data = 'Cora'\n",
        "dataset = Planetoid(root= '/tmp/' + name_data, name = name_data)\n",
        "dataset.transform = T.NormalizeFeatures() #which row‑normalizes each node’s feature vector to unit sum (For each node, take its feature vector and scale it so that the sum of all feature values equals 1.)\n",
        "\n",
        "print(f\"Number of Classes in {name_data}:\", dataset.num_classes)\n",
        "print(f\"Number of Node Features in {name_data}:\", dataset.num_node_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSl9phx_T24E",
        "outputId": "e0ce2949-5fc7-484b-a2e9-907ca3c0a348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Classes in Cora: 7\n",
            "Number of Node Features in Cora: 1433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GAT, self).__init__()\n",
        "    self.hid = 8 #each head in layer 1 outputs an 8‑dim vector.\n",
        "    self.in_head = 8 #number of attention heads in the first layer.\n",
        "    self.out_head = 1\n",
        "    #transforms 1433 -> (8 * 8)=64 dims, using 8 parallel attention heads + 0.6 dropout - in and before GATConv\n",
        "    self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
        "    #merges those 64 dims back down to 7 classes (no concatenation, single head), again with dropout.\n",
        "    self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False, heads=self.out_head, dropout=0.6)\n",
        "\n",
        "\"\"\"Dropout is a regularization technique used during training of neural networks to prevent overfitting.\n",
        "It works by randomly \"dropping out\" (i.e., setting to zero) a fraction of the input values during each forward pass.\"\"\"\n",
        "\n",
        "  def forward(self, data):\n",
        "    x, edge_index = data.x, data.edge_index #Input: node features x (shape [N,1433]) and edge_index adjacency\n",
        "    x = F.dropout(x, p=0.6, training=self.training)\n",
        "    x = self.conv1(x, edge_index) #attention‑based neighborhood aggregation\n",
        "    x = F.elu(x) #Non linearity\n",
        "    x = F.dropout(x, p=0.6, training=self.training)\n",
        "    x = self.conv2(x, edge_index) #final transform to 7‑dim logits per node.\n",
        "    return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "URLULNiWT4Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = \"cpu\"\n",
        "model = GAT().to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_b3yivdUTRk",
        "outputId": "a783f38e-31c8-4e36-bc5a-d0c3d493c0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GAT(\n",
              "  (conv1): GATConv(1433, 8, heads=8)\n",
              "  (conv2): GATConv(64, 7, heads=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0].to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "fq5XWFxO2iYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(1000):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  out = model(data)\n",
        "  loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "  if epoch%200 == 0:\n",
        "    print(loss)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15XmQS4eUVJY",
        "outputId": "938bc6a4-6e2c-43fb-98b3-d4e3b9966ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.9453, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8359, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5636, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4769, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4388, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "_, pred = model(data).max(dim=1)\n",
        "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "acc = correct / data.test_mask.sum().item()\n",
        "print('Accuracy: {:.4f}'.format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM0SEWOTUjbI",
        "outputId": "8b82c671-ca3a-44ed-8cf3-aec8be9e2a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.nn import TransformerConv\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "#Load Cora dataset\n",
        "dataset = Planetoid(root='data/Cora', name='Cora', transform=NormalizeFeatures())\n",
        "data = dataset[0]"
      ],
      "metadata": {
        "id": "cUffAXA92M1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a simple 2-layer Transformer-style GNN\n",
        "class GraphTransformerNet(torch.nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, num_classes, heads=4):\n",
        "    super().__init__()\n",
        "    self.conv1 = TransformerConv(in_channels, hidden_channels, heads=heads, concat=True, dropout=0.1)\n",
        "    self.conv2 = TransformerConv(hidden_channels * heads, num_classes, heads=1, concat=False, dropout=0.1)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.elu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "vlUAgHx-iN4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GraphTransformerNet(dataset.num_features, hidden_channels=8, num_classes=dataset.num_classes).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zACpctbclpk0",
        "outputId": "9ee1f16f-c6dd-4ec9-d7fa-29c2cdf35bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GraphTransformerNet(\n",
              "  (conv1): TransformerConv(1433, 8, heads=4)\n",
              "  (conv2): TransformerConv(32, 7, heads=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "Q-rCVkvc2oQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(1, 201):\n",
        "  optimizer.zero_grad()\n",
        "  out = model(data.x, data.edge_index)\n",
        "  loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 50 == 0:\n",
        "    print(f'Epoch {epoch:03d}, Loss: {loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIgcdUsP2vR6",
        "outputId": "74b41a52-0545-4ecc-803e-6123b16a9d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 050, Loss: 0.5930\n",
            "Epoch 100, Loss: 0.1913\n",
            "Epoch 150, Loss: 0.1368\n",
            "Epoch 200, Loss: 0.0958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "preds = model(data.x, data.edge_index).argmax(dim=-1)\n",
        "accs = []\n",
        "for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "  accs.append((preds[mask] == data.y[mask]).sum().item() / mask.sum().item())\n",
        "print(f'Train Acc: {accs[0]:.4f}, Val Acc: {accs[1]:.4f}, Test Acc: {accs[2]:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LtTg-0-2w_O",
        "outputId": "a2846307-1a5a-42c6-d41f-447e1367fb86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 1.0000, Val Acc: 0.7700, Test Acc: 0.7880\n"
          ]
        }
      ]
    }
  ]
}